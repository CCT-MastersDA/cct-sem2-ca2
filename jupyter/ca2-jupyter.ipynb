{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacfc498-7e48-4746-8e89-8edf314fa37c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Auxiliary Functions\n",
    "\n",
    "In this section the auxiliary functions used in this project are implemented.\n",
    "\n",
    "For this part, 2 Python helper modules were also implemented under jupyter/modules folder:\n",
    "\n",
    "    - TextProcessor: Which has the methods to process the text using the approaches used in this project.\n",
    "    - JsonHelper: It was used to convert dictionaries into JSON format.\n",
    "    \n",
    "The reason to separate these modules from the Jupyter notebooks was to keep the code organised and to follow the best programming practices with regards to reuse and code modularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c5267142-da72-43a4-9afc-166c8334b0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef process_tweet(tweet):\\n    tweet_raw = tweet\\n    sentiment_raw = TextProcessor.get_sentiment(tweet_raw)\\n\\n    tweet_str = TextProcessor.clean_text(tweet_raw)\\n    sentiment_str = TextProcessor.get_sentiment(tweet_str)\\n\\n    tweet_clr = TextProcessor.process_text(tweet_raw)\\n    sentiment_clr = TextProcessor.get_sentiment(tweet_clr)\\n\\n    tweet_st = TextProcessor.process_text(tweet_raw, use_stemmer=True)\\n    sentiment_st = TextProcessor.get_sentiment(tweet_st)\\n\\n    tweet_lm = TextProcessor.process_text(tweet_raw, use_lemmatizer=True)\\n    sentiment_lm = TextProcessor.get_sentiment(tweet_lm)\\n\\n    return (tweet_raw, sentiment_raw, tweet_str, sentiment_str, tweet_clr, sentiment_clr, tweet_st, sentiment_st, tweet_lm, sentiment_lm)\\n\\ndef process_tweets_async(tweets_list):\\n    proc_tweets = []\\n    with concurrent.futures.ThreadPoolExecutor() as executor:\\n        #results = executor.map(process_tweet, tweets_list)\\n        results = list(tqdm(executor.map(process_tweet, tweets_list), total=len(tweets_list), desc='Processing'))\\n        for result in results:\\n            proc_tweets.append(result)\\n\\n    tweets_df = pd.DataFrame(proc_tweets, columns=['tweet_raw', 'sent_raw', 'tweet_str', 'sent_str', 'tweet_clr', 'sent_clr', 'tweet_st', 'sent_st', 'tweet_lm', 'sent_lm'])\\n\\n    return tweets_df\\n\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding modules folder to the path\n",
    "import sys\n",
    "sys.path.append('./modules')\n",
    "\n",
    "# importing constants and modules created for this project\n",
    "from constants import *\n",
    "from text_processor import *\n",
    "\n",
    "# importing modules specific to this notebook\n",
    "import logging\n",
    "import pandas as pd\n",
    "#import altair as alt\n",
    "#import seaborn as sns\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from dotenv import dotenv_values\n",
    "from nltk.probability import FreqDist\n",
    "from tqdm import tqdm\n",
    "\n",
    "# use a logger to help debugging\n",
    "logger = logging.getLogger('ca2-jupyter')\n",
    "\n",
    "# set logger level\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "def get_freq(df, colname):\n",
    "    '''\n",
    "    Auxiliary function to calculate the frequency distribution of the words in a column.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): Data frame to be processed.\n",
    "        colname  (str): Column name to get its frequency calculated.\n",
    "    Returns:\n",
    "        fdist    (mtx): Frequency distribution of the tokens.\n",
    "    '''\n",
    "    tokens_lst = []\n",
    "    for i in range(0, len(df)):\n",
    "        text = df.loc[i][colname]\n",
    "        tokens_lst+= TextProcessor.get_tokens(text)\n",
    "\n",
    "    fdist = FreqDist(tokens_lst)\n",
    "\n",
    "    return fdist\n",
    "\n",
    "def process_tweets(tweets_list):\n",
    "    \"\"\"\n",
    "    Auxiliary method to process the tweets from the input list.\n",
    "    \n",
    "    It processes the tweets, extracting their sentiment during the process.\n",
    "    \n",
    "    It generates a table with the following columns:\n",
    "\n",
    "        'tweet_raw', 'sent_raw', 'tweet_clr', 'sent_clr', 'tweet_st', 'sent_st', 'tweet_lm', 'sent_lm'\n",
    "    \n",
    "    Parameters:\n",
    "        tweets_list (lst): List of tweets to be processed.\n",
    "    Returns:\n",
    "        tweets_df (DataFrame): Data frame with the processed data.\n",
    "    \"\"\"\n",
    "    proc_tweets = []\n",
    "    for i in range(0, len(tweets_list)):\n",
    "        # raw tweet\n",
    "        tweet_raw = tweets_list[i]\n",
    "        sentiment_raw = TextProcessor.get_sentiment(tweet_raw)\n",
    "\n",
    "        # strip tweet\n",
    "        tweet_str = TextProcessor.clean_text(tweet_raw)\n",
    "        sentiment_str = TextProcessor.get_sentiment(tweet_str)\n",
    "        \n",
    "        # cleaned tweet\n",
    "        tweet_clr = TextProcessor.process_text(tweet_raw)\n",
    "        sentiment_clr = TextProcessor.get_sentiment(tweet_clr)\n",
    "\n",
    "        # steemed tweet\n",
    "        tweet_st = TextProcessor.process_text(tweet_raw, use_stemmer=True)\n",
    "        sentiment_st = TextProcessor.get_sentiment(tweet_st)\n",
    "\n",
    "        # lemmatized tweet\n",
    "        tweet_lm = TextProcessor.process_text(tweet_raw, use_lemmatizer=True)\n",
    "        sentiment_lm = TextProcessor.get_sentiment(tweet_lm)\n",
    "\n",
    "        proc_tweets.append((tweet_raw, sentiment_raw, tweet_str, sentiment_str, tweet_clr, sentiment_clr, tweet_st, sentiment_st, tweet_lm, sentiment_lm))\n",
    "\n",
    "    tweets_df = pd.DataFrame(proc_tweets, columns=['tweet_raw', 'sent_raw', 'tweet_str', 'sent_str', 'tweet_clr', 'sent_clr', 'tweet_st', 'sent_st', 'tweet_lm', 'sent_lm'])\n",
    "\n",
    "    return tweets_df\n",
    "\n",
    "\n",
    "def process_tweets_chunk(chunk):\n",
    "    proc_tweets = []\n",
    "    for tweet in chunk:\n",
    "        tweet_raw = tweet\n",
    "        sentiment_raw = TextProcessor.get_sentiment(tweet_raw)\n",
    "\n",
    "        tweet_str = TextProcessor.clean_text(tweet_raw)\n",
    "        sentiment_str = TextProcessor.get_sentiment(tweet_str)\n",
    "\n",
    "        tweet_clr = TextProcessor.process_text(tweet_raw)\n",
    "        sentiment_clr = TextProcessor.get_sentiment(tweet_clr)\n",
    "\n",
    "        tweet_st = TextProcessor.process_text(tweet_raw, use_stemmer=True)\n",
    "        sentiment_st = TextProcessor.get_sentiment(tweet_st)\n",
    "\n",
    "        tweet_lm = TextProcessor.process_text(tweet_raw, use_lemmatizer=True)\n",
    "        sentiment_lm = TextProcessor.get_sentiment(tweet_lm)\n",
    "\n",
    "        proc_tweets.append((tweet_raw, sentiment_raw, tweet_str, sentiment_str, tweet_clr, sentiment_clr, tweet_st, sentiment_st, tweet_lm, sentiment_lm))\n",
    "\n",
    "    return proc_tweets\n",
    "\n",
    "def process_tweets_async(tweets_list, chunk_size=1000):\n",
    "    proc_tweets = []\n",
    "    num_chunks = (len(tweets_list) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for i in range(num_chunks):\n",
    "            chunk = tweets_list[i*chunk_size:(i+1)*chunk_size]\n",
    "            future = executor.submit(process_tweets_chunk, chunk)\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=num_chunks, desc='Processing'):\n",
    "            proc_tweets.extend(future.result())\n",
    "\n",
    "    tweets_df = pd.DataFrame(proc_tweets, columns=['tweet_raw', 'sent_raw', 'tweet_str', 'sent_str', 'tweet_clr', 'sent_clr', 'tweet_st', 'sent_st', 'tweet_lm', 'sent_lm'])\n",
    "\n",
    "    return tweets_df\n",
    "\n",
    "'''\n",
    "def process_tweet(tweet):\n",
    "    tweet_raw = tweet\n",
    "    sentiment_raw = TextProcessor.get_sentiment(tweet_raw)\n",
    "\n",
    "    tweet_str = TextProcessor.clean_text(tweet_raw)\n",
    "    sentiment_str = TextProcessor.get_sentiment(tweet_str)\n",
    "\n",
    "    tweet_clr = TextProcessor.process_text(tweet_raw)\n",
    "    sentiment_clr = TextProcessor.get_sentiment(tweet_clr)\n",
    "\n",
    "    tweet_st = TextProcessor.process_text(tweet_raw, use_stemmer=True)\n",
    "    sentiment_st = TextProcessor.get_sentiment(tweet_st)\n",
    "\n",
    "    tweet_lm = TextProcessor.process_text(tweet_raw, use_lemmatizer=True)\n",
    "    sentiment_lm = TextProcessor.get_sentiment(tweet_lm)\n",
    "\n",
    "    return (tweet_raw, sentiment_raw, tweet_str, sentiment_str, tweet_clr, sentiment_clr, tweet_st, sentiment_st, tweet_lm, sentiment_lm)\n",
    "\n",
    "def process_tweets_async(tweets_list):\n",
    "    proc_tweets = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        #results = executor.map(process_tweet, tweets_list)\n",
    "        results = list(tqdm(executor.map(process_tweet, tweets_list), total=len(tweets_list), desc='Processing'))\n",
    "        for result in results:\n",
    "            proc_tweets.append(result)\n",
    "\n",
    "    tweets_df = pd.DataFrame(proc_tweets, columns=['tweet_raw', 'sent_raw', 'tweet_str', 'sent_str', 'tweet_clr', 'sent_clr', 'tweet_st', 'sent_st', 'tweet_lm', 'sent_lm'])\n",
    "\n",
    "    return tweets_df\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a847bcb-cebd-4e49-b613-8ae891889359",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "In this section a sentiment analysis is performed over the tweets dataset using time-series to forcast the sentiment of tweets over a period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6740df2-21d0-43f7-8eef-bb20a348cd40",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Since this dataset was used for sentiment analysis, the following operations were performed on the tweets text as part of EDA:\n",
    "\n",
    "1. Removal of special characters, links and images (tweet_str)\n",
    "2. Application of step 1 + Removal of stop words (tweet_clr)\n",
    "3. Application of step 2 + Lemmatizer technique (tweet_lm)\n",
    "4. Application of step 2 + Stemmer techinique (tweet_st)\n",
    "5. Extract the sentiment from each version of the tweets\n",
    "\n",
    "Each tweet version had its own sentiment calculated, because it was observed that the sentiment algorithm provides different results for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f45c6c44-a8e0-4ee8-a601-1a7b218cfbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting tweets from the archive\n",
    "dataset = pd.read_csv(TWEETS_DS_RAW, encoding='utf-8')\n",
    "\n",
    "# dataset column names\n",
    "tweets_cols = ['username', 'location', 'tweetid', 'text', 'hashtags', 'language', 'extractedts']\n",
    "\n",
    "# storing the dataset in a variable for processing\n",
    "tweets_df = dataset[tweets_cols]\n",
    "\n",
    "# converting date column into proper date type\n",
    "tweets_df['extractedts'] = pd.to_datetime(tweets_df['extractedts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d2339d57-0a61-444d-abb5-5938e1a243d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2022-04-01 00:01:44.294934'),\n",
       " Timestamp('2022-04-02 00:46:57.116538'))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the date covereage in this dataset\n",
    "tweets_df['extractedts'].min(), tweets_df['extractedts'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddb10fcf-2cb5-4a49-aa1a-bf87e83b7e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>location</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>language</th>\n",
       "      <th>extractedts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>364875</td>\n",
       "      <td>212933</td>\n",
       "      <td>3.648750e+05</td>\n",
       "      <td>364875</td>\n",
       "      <td>364875</td>\n",
       "      <td>364875</td>\n",
       "      <td>364875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>166400</td>\n",
       "      <td>42667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104548</td>\n",
       "      <td>85962</td>\n",
       "      <td>61</td>\n",
       "      <td>364875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>FuckPutinBot</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>⚡The Ukrainian Air Force would like to address...</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-01 00:44:20.097867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>789</td>\n",
       "      <td>3285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7297</td>\n",
       "      <td>72345</td>\n",
       "      <td>254626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-01 00:01:44.294934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-04-02 00:46:57.116538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509875e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.829062e+13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509682e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509795e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509880e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509958e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.510044e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            username location       tweetid  \\\n",
       "count         364875   212933  3.648750e+05   \n",
       "unique        166400    42667           NaN   \n",
       "top     FuckPutinBot  Ukraine           NaN   \n",
       "freq             789     3285           NaN   \n",
       "first            NaN      NaN           NaN   \n",
       "last             NaN      NaN           NaN   \n",
       "mean             NaN      NaN  1.509875e+18   \n",
       "std              NaN      NaN  9.829062e+13   \n",
       "min              NaN      NaN  1.509682e+18   \n",
       "25%              NaN      NaN  1.509795e+18   \n",
       "50%              NaN      NaN  1.509880e+18   \n",
       "75%              NaN      NaN  1.509958e+18   \n",
       "max              NaN      NaN  1.510044e+18   \n",
       "\n",
       "                                                     text hashtags language  \\\n",
       "count                                              364875   364875   364875   \n",
       "unique                                             104548    85962       61   \n",
       "top     ⚡The Ukrainian Air Force would like to address...       []       en   \n",
       "freq                                                 7297    72345   254626   \n",
       "first                                                 NaN      NaN      NaN   \n",
       "last                                                  NaN      NaN      NaN   \n",
       "mean                                                  NaN      NaN      NaN   \n",
       "std                                                   NaN      NaN      NaN   \n",
       "min                                                   NaN      NaN      NaN   \n",
       "25%                                                   NaN      NaN      NaN   \n",
       "50%                                                   NaN      NaN      NaN   \n",
       "75%                                                   NaN      NaN      NaN   \n",
       "max                                                   NaN      NaN      NaN   \n",
       "\n",
       "                       extractedts  \n",
       "count                       364875  \n",
       "unique                      364875  \n",
       "top     2022-04-01 00:44:20.097867  \n",
       "freq                             1  \n",
       "first   2022-04-01 00:01:44.294934  \n",
       "last    2022-04-02 00:46:57.116538  \n",
       "mean                           NaN  \n",
       "std                            NaN  \n",
       "min                            NaN  \n",
       "25%                            NaN  \n",
       "50%                            NaN  \n",
       "75%                            NaN  \n",
       "max                            NaN  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describing the data\n",
    "tweets_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8444312b-260f-4d61-ae99-dec0e9cecae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the tweets list from the dataframe\n",
    "tweets_list = tweets_df['text'][:2000].values\n",
    "len(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4b5cc2-ce0c-4006-adb5-cc7dd5da7c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                               | 264/365 [39:26<05:15,  3.12s/it]"
     ]
    }
   ],
   "source": [
    "# getting the tweets list from the dataframe\n",
    "tweets_list = tweets_df['text'].values\n",
    "\n",
    "# processing retrieved data using text processing\n",
    "tweets_proc = process_tweets_async(tweets_list)\n",
    "\n",
    "tweets_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4db9d9f1-08b6-4272-b8f8-3cc2ca2063d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc8164-73e4-44b8-bf67-2be620b6e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the tweets list from the dataframe\n",
    "tweets_list = tweets_df['text'].values\n",
    "\n",
    "# processing retrieved data using text processing\n",
    "tweets_proc = process_tweets(tweets_list)\n",
    "\n",
    "# concat result with exiting dataframe\n",
    "tweets_df = pd.concat([tweets_df, tweets_proc])\n",
    "\n",
    "# storing the processed data in the datasets/tweets-online folder\n",
    "tweets_df.to_csv(get_collected_twitter_file_path())\n",
    "\n",
    "# show the tweets collected\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d7dcb8-3dfe-40d4-a839-7335110f6474",
   "metadata": {},
   "source": [
    "## Tweets Dataset Visualization\n",
    "\n",
    "In order to visualize this dataset, it was prepared a frequency graph to show the most common words.\n",
    "\n",
    "Frequency graphs and word clouds are common ways to visualize text datasets, so it is possible to have an idea of its main content.\n",
    "\n",
    "For this purpose it was used a tokenizer algorithm to generate tags from all the tweets after the cleanning stage.\n",
    "\n",
    "The frequency was the same if Stemmer or Lemmatizer techniques were applied, so the following graph only shows the frequency of the cleaned tweet data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8e80942-8b93-4e04-9c85-bcd4cabb4320",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tweet_clr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tweet_clr'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# display the frequency distribution\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fdist \u001b[38;5;241m=\u001b[39m \u001b[43mget_freq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweets_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet_clr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m g_plot \u001b[38;5;241m=\u001b[39m fdist\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;241m30\u001b[39m, cumulative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord Frequency of the Tweets Dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# saving graph as an image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[37], line 37\u001b[0m, in \u001b[0;36mget_freq\u001b[1;34m(df, colname)\u001b[0m\n\u001b[0;32m     35\u001b[0m tokens_lst \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[1;32m---> 37\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     38\u001b[0m     tokens_lst\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m TextProcessor\u001b[38;5;241m.\u001b[39mget_tokens(text)\n\u001b[0;32m     40\u001b[0m fdist \u001b[38;5;241m=\u001b[39m FreqDist(tokens_lst)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tweet_clr'"
     ]
    }
   ],
   "source": [
    "# display the frequency distribution\n",
    "fdist = get_freq(tweets_df, 'tweet_clr')\n",
    "g_plot = fdist.plot(30, cumulative=False, title='Word Frequency of the Tweets Dataset')\n",
    "\n",
    "# saving graph as an image\n",
    "fig = g_plot.get_figure()\n",
    "fig.savefig(join(IMAGES_FOLDER, GRAPH_WORD_FREQ), format='png')\n",
    "\n",
    "# showing the image\n",
    "g_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b7c201-51de-4efd-88c1-9500581b4a29",
   "metadata": {},
   "source": [
    "In the next step the tweets dataset are pre-processed for the sentiment analysis and time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e11b19-354e-4c76-8b6b-c801f415ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting date column into proper date type\n",
    "dataset['extractedts'] = pd.to_datetime(dataset['extractedts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4215929-fa32-46d2-b39c-577b4aad0e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2022-04-01 00:01:44.294934', '2022-04-02 00:46:57.116538')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the date covereage in this dataset\n",
    "dataset['extractedts'].min(), dataset['extractedts'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7b23759-66ce-4102-8026-8fda4b1a8273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>location</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>language</th>\n",
       "      <th>extractedts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>364875</td>\n",
       "      <td>212933</td>\n",
       "      <td>3.648750e+05</td>\n",
       "      <td>364875</td>\n",
       "      <td>364875</td>\n",
       "      <td>364875</td>\n",
       "      <td>364875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>166400</td>\n",
       "      <td>42667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104548</td>\n",
       "      <td>85962</td>\n",
       "      <td>61</td>\n",
       "      <td>364875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>FuckPutinBot</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>⚡The Ukrainian Air Force would like to address...</td>\n",
       "      <td>[]</td>\n",
       "      <td>en</td>\n",
       "      <td>2022-04-01 00:44:20.097867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>789</td>\n",
       "      <td>3285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7297</td>\n",
       "      <td>72345</td>\n",
       "      <td>254626</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509875e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.829062e+13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509682e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509795e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509880e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.509958e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.510044e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            username location       tweetid  \\\n",
       "count         364875   212933  3.648750e+05   \n",
       "unique        166400    42667           NaN   \n",
       "top     FuckPutinBot  Ukraine           NaN   \n",
       "freq             789     3285           NaN   \n",
       "mean             NaN      NaN  1.509875e+18   \n",
       "std              NaN      NaN  9.829062e+13   \n",
       "min              NaN      NaN  1.509682e+18   \n",
       "25%              NaN      NaN  1.509795e+18   \n",
       "50%              NaN      NaN  1.509880e+18   \n",
       "75%              NaN      NaN  1.509958e+18   \n",
       "max              NaN      NaN  1.510044e+18   \n",
       "\n",
       "                                                     text hashtags language  \\\n",
       "count                                              364875   364875   364875   \n",
       "unique                                             104548    85962       61   \n",
       "top     ⚡The Ukrainian Air Force would like to address...       []       en   \n",
       "freq                                                 7297    72345   254626   \n",
       "mean                                                  NaN      NaN      NaN   \n",
       "std                                                   NaN      NaN      NaN   \n",
       "min                                                   NaN      NaN      NaN   \n",
       "25%                                                   NaN      NaN      NaN   \n",
       "50%                                                   NaN      NaN      NaN   \n",
       "75%                                                   NaN      NaN      NaN   \n",
       "max                                                   NaN      NaN      NaN   \n",
       "\n",
       "                       extractedts  \n",
       "count                       364875  \n",
       "unique                      364875  \n",
       "top     2022-04-01 00:44:20.097867  \n",
       "freq                             1  \n",
       "mean                           NaN  \n",
       "std                            NaN  \n",
       "min                            NaN  \n",
       "25%                            NaN  \n",
       "50%                            NaN  \n",
       "75%                            NaN  \n",
       "max                            NaN  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describing the data\n",
    "dataset.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282a0c3c-45ba-4a96-a968-b222d53361c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "file_path = './datasets/ukraine-dataset.csv'\n",
    "date_column_index = 17  # Index of the column containing dates (adjust if necessary)\n",
    "\n",
    "dates = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip header row if present\n",
    "    for row in reader:\n",
    "        date_str = row[date_column_index]\n",
    "        try:\n",
    "            date = datetime.strptime(date_str, \"%Y-%m-%d\")  # Adjust date format if necessary\n",
    "            dates.append(date)\n",
    "        except ValueError:\n",
    "            print(f\"Invalid date format: {date_str}\")\n",
    "\n",
    "if dates:\n",
    "    min_date = min(dates)\n",
    "    max_date = max(dates)\n",
    "    print(\"Date Range:\")\n",
    "    print(\"Min Date:\", min_date.date())\n",
    "    print(\"Max Date:\", max_date.date())\n",
    "else:\n",
    "    print(\"No dates found in the CSV file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39639895-39af-4216-be9d-712a804790b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
